# (APPENDIX) Appendices {-} 


# Ring theory {#ring-theory}

A ring is a slightly more complex algebraic structure, since it is
equipped with two operations, the first of them with the exact same
properties of a commutative group operation, whereas the second does not
require the existence of inverse.

::: {.definition}
*A* ring[^42] *is a triple $(\mathbb{K},\circ,\ast)$,
where $\mathbb{K}$ is a set and $\circ$ and $\ast$ are operations on
$\mathbb{K}$, that is, functions $$\begin{aligned}
        \circ : \phantom{a} & \mathbb{K}\times \mathbb{K}& \rightarrow & \phantom{a}\mathbb{K}& \qquad\qquad & \ast :& \mathbb{K}\times\mathbb{K}& \rightarrow & \mathbb{K}\\
                & (x,y) & \mapsto & \phantom{a} x\circ y, & & & (x,y) & \mapsto & x\ast y,
    \end{aligned}$$ which additionally satisfy the following
properties:*

1.  *$(\mathbb{K},\circ)$ is a commutative group.*

2.  *$(\mathbb{K},\ast)$ satisfies the following properties:*

    1.  Associative law: $(x\ast y)\ast z = x\ast(y\ast z)$ *for all*
        $x,y,z\in \mathbb{K}$.

    2.  Existence of identity: *there exists $e\in \mathbb{K}$ such
        that $e\ast x=x\ast e=x$ for all $x\in \mathbb{K}$. Such element
        $e$ is called the* identity element *of $\ast$.*

    3.  Commutativity: $x\ast y=y \ast x$ *for all*
        $x,y\in \mathbb{K}$.

3.  Distributive law: $x\ast(y\circ z)=(x\ast y)\circ(x\ast z)$ *for
    all* $x,y,z\in\mathbb{K}.$*
:::

Since we have two operations at the same time, we will adopt the
additive and multiplicative notation from groups to represent each of
them, respectively. That is, we will think of the first operation of a
ring as a form of *addition* and the second as a form of
*multiplication*. On input $x,y$, we write the result of the first
operation by $x+y$, and the result of the second by $xy$. The identity
elements of each operation are denoted by $0$ and $1$, respectively. The
inverse of $x$ with respect to the first operation is denoted by $-x$.
The inverse of $x$ with respect to the second operation, if exists, is
denoted by $x^{-1}$. The following table summarizes the notation:

::: {.center}
   Operation   Operation on input $x,y$   Identity element   Inverse of $x$
  ----------- -------------------------- ------------------ ----------------
      $+$               $x+y$                   $0$               $-x$
    $\cdot$              $xy$                   $1$             $x^{-1}$
:::

We consider some examples:

-   $(\mathbb{Z},+,\cdot)$, with usual integer addition and
    multiplication, is a ring, since $(\mathbb{Z},+)$ is a group with
    identity $0$, $(\mathbb{Z},\cdot)$ verifies associativity,
    commutativity and existence of identity ($1$), and it is easy to
    verify distributivity. Note, however, that $\mathbb{Z}$ does not
    contain multiplicative inverses, since for example there is no
    $x\in\mathbb{Z}$ such that $2x=1$. For similar reasons,
    $(\mathbb{Z}_n,+,\cdot)$, with modular addition and multiplication,
    is a ring, for $n\in\mathbb{N}$.

-   $(\mathbb{Q}[X],+,\cdot)$, where $\mathbb{Q}[X]$ is the set of
    polynomials with rational coefficients in variable $X$, $+$ is
    point-wise addition, and $\cdot$ is point-wise multiplication, is
    also a ring. The identity elements are the constant polynomials
    $p(X)=0$ and $q(X)=1$, respectively.

Consider the ring $\mathbb{Z}_9$, with addition and multiplication
modulo $9$. The following table gives the multiplicative inverses of
each element:

::: {.center}
         $x$  $0$   $1$   $2$   $3$   $4$   $5$   $6$   $7$   $8$
  ---------- ----- ----- ----- ----- ----- ----- ----- ----- -----
    $x^{-1}$  $-$   $1$   $5$   $-$   $7$   $2$   $-$   $4$   $8$
:::

Note that some elements are not invertible, in this case $0$, $3$ and
$6$. Those elements in a ring that happen to have a multiplicative
inverse receive a special name.

::: {.definition}
*Let $\mathbb{K}$ be a ring. An element
$x\in\mathbb{K}$ that has a multiplicative inverse is called a* unit*.
We denote the set of units of $\mathbb{K}$ by $\mathbb{K}^*$.*
:::

Note that the only thing that $(\mathbb{K},\cdot)$ was missing to be a
group is existence of inverses. By restricting ourselves to the subset
$\mathbb{K}^*$ of invertible elements, we have that
$(\mathbb{K}^*,\cdot)$ is a group. Note that, in the previous lesson, we
defined $\mathbb{Z}_n^*$ to be the set of invertible elements of
$\mathbb{Z}_n$, so in this new formulation, we can state that
$\mathbb{Z}_n^*$ is actually the set of units of $\mathbb{Z}_n$.
Moreover, we now know that $\mathbb{Z}_n^*$ is a group with
multiplication modulo $n$.

In light of the discussion above, we start this section by asking
ourselves the following question.

::: {.problem}
*Is it possible to have a ring $\mathbb{K}$ in which all
the elements are invertible?*
:::

This will never be the case, unless the ring only contains one element,
since the additive identity $0$ cannot have a multiplicative inverse
otherwise. To see this, assume that there exists $x\in\mathbb{K}$ such
that $0\cdot x = 1$. First, $$0x=(1+(-1))x=x+(-x)=0,$$ using the
distributive law and the properties of the multiplicative identity $1$.
Then we have that $1=0$. But then, this means that $$x=1x=0x=0,$$ using
the properties of the multiplicative and additive identities. Thus, we
conclude that every element is the same. And rings with just one element
are not very interesting.

So, except for the trivial case, we know that $0$ cannot have an
inverse. What about the rest of the elements? In this case, the answer
is affirmative: there exist rings in which every non-zero element is
invertible, and we call those *fields*.

::: {.definition}
*A* field *is a ring in which every non-zero element
has a multiplicative inverse in the ring.*
:::

# Primality testing {#primality-testing}

Many modern cryptographic schemes require some very large prime numbers,
so we are interested in generating these numbers efficiently. The basic
strategy is very simple: pick a random number of the desired size, and
check whether it is prime. If it is not, pick another. A consequence of
the prime number theorem
(Proposition \@ref(prp:pnt)) is that, on average, we need about $\lambda$
tries to find a prime of bitlength $\lambda$, so that's not too bad.

But what about recognizing whether a number is prime or not? This part
is solved with a *primality test*. In this section, we look at some of
them.

The naive way to check primality is to check if the candidate integer
$n=O(2^\lambda)$ is divisible by any other smaller integer. If that's
not the case, then necessarily $n$ is prime. So, in general, this
algorithm runs in time $O(n)$, which is exponential in $\lambda$. We can
do slightly better by observing that it is enough to check divisors up
to $\sqrt{n}$.

```{proposition}
*Let $n\in\mathbb{N}$. If $n$ is composite, then $n$
has a non-trivial divisor $d\leq\sqrt{n}$.*
```

::: {.proof}
We prove the result by contradiction. Assume that $n$ is
composite and has no non-trivial divisor smaller than $\sqrt{n}$. Since
$n$ is composite, it has at least two non-trivial divisors
$a,b\in\mathbb{N}$, such that $ab=n$. But every divisor is larger than
$\sqrt{n}$, therefore $$n=ab>\sqrt{n}\cdot\sqrt{n}=n,$$ and thus $n>n$,
which is a contradiction.
:::

Still, our primality test would run in time $O(\sqrt{n})$, which is
still exponential in $\lambda$. We need to look for a more efficient
approach. We turn our attention to Fermat's little theorem:

```{proposition, name="Fermat's little theorem"}
*Let $p$ be a prime
number. Then, for any $x\in\mathbb{Z}$ such that $p\nmid x$, we have
that $$x^{p-1}\equiv1\pmod{p}.$$*
```

This theorem tells us about a condition that all prime numbers verify,
which gives us an easy condition to test and discard composite numbers.
Given a candidate $n$, choose some $x\in\mathbb{Z}$ such that
$\gcd(n,x)=1$, and compute $$x^{n-1}\bmod{n}.$$ If the result is not
$1$, then we conclude that $n$ is not a prime. This is known as the
*Fermat primality test*, and it is very efficient. But what happens if
the result is $1$? Does this mean that $n$ is a prime? Unfortunately, it
is not as simple as this. For example, take $n=91$ and $x=3$. It is easy
to verify that $$3^{90}\pmod{91}=1,$$ but $91=7\cdot13$, so it is not a
prime. We call the numbers that produce these "false positives"
*pseudoprimes*.

::: {.definition}
*Let $n\in\mathbb{Z}$ be a composite number, and let
$x\in\mathbb{Z}$ such that $\gcd(x,n)=1$. We call $n$ an* $x$-Fermat
pseudoprime *if* $$x^{n-1}\equiv1\pmod{n}.$$
:::

Moreover, for any $x$ there are many $x$-Fermat pseudoprimes. Okay, so
everybody don't panic. Fermat's little theorem tell us that primes
verify the condition $$x^{p-1}\equiv1\pmod{p}$$ for *every* $x$ such
that $\gcd(p,x)=1$. So surely we can find some other base $x$ so that
$91$ cannot fool the test. Indeed, $$5^{90}\bmod{91} = 64.$$ So that's
it, we are now sure that $91$ is not a prime, because it does not pass
the test for $x=5$. Problem solved.

Or is it? What if there are some truly evil numbers that can fool the
Fermat test for all the possible bases?

::: {.definition}
*Let $n\in\mathbb{Z}$ be a composite number. We say
that $n$ is a* Carmichael number *if it is a $x$-Fermat pseudoprime for
every $x\in\mathbb{Z}$ such that* $\gcd(n,x)=1$.
:::

Unfortunately, there are too many of these Carmichael numbers. For large
values of $B\in\mathbb{N}$, the number of Carmichael numbers up to $B$
is approximately $B^{2/7}$. This means that a significant amount of
numbers will fool our test, even if we run it for all the possible
bases. We need a better test.

The solution lies on a strengthened version of Fermat's little theorem.

```{proposition}
*Let $p\in\mathbb{N}$ be an odd prime, and let
$p-1=2^st$ such that $2\nmid t$. If $\gcd(p,x)=1$, then either
$$x^t\equiv1\pmod{p},$$ or there exists $i\in\{0,1,\dots,s-1\}$ such
that $$x^{2^it}\equiv-1\pmod{p}.$$*
```

Again, the idea is to test our numbers against this result, and see if
they satisfy the equations. This version is called the *strong Fermat
primality test*. Observe that, in the worst case, we need to perform $s$
checks, and if $p$ has bitlength $\lambda$, then $s=O(\lambda)$, and
each check amounts to modular exponentiation, so overall this is
efficient. As before, this new test also produces false positives.

::: {.definition}
*Let $n\in\mathbb{Z}$ be an odd composite integer,
and let $n-1=2^st$. Let $x\in\mathbb{Z}$ such that $\gcd(x,n)=1$. We
call $n$ a* strong $x$-Fermat pseudoprime *if
$$x^{t-1}\equiv1\pmod{n},$$ or there exists $i\in\{0,1,\dots,s-1\}$ such
that* $$x^{2^it}\equiv-1\pmod{n}.$$
:::

::: {.exercise}
*Check that $2047$ is a strong $2$-Fermat pseudoprime,
because it satisfies the first condition, and that $91$ is a strong
$10$-Fermat pseudoprime, because it satisfies the second condition.*
:::

So why bother with the strong Fermat test, if it has the same flaw? The
key point is that, even if we have strong pseudoprimes, the strong
Fermat test does not have its version of Carmichael numbers. That is,
for any composite number $n$ we will always be able to find a base such
that $n$ does not pass the strong test, showing that it is indeed
composite. Moreover, most of the bases will do!

```{proposition, name="Monier--Rabin"}
*Let $n>9$ be an odd composite
integer. Then, the number of bases $x$ smaller than $n$ such that $n$ is
a strong $x$-Fermat pseudoprime is at most $\frac14\varphi(n)$.*
```

This is a very strong result. Since $\varphi(n)<n$, this tells us that,
by picking a base in $\mathbb{Z}_n$ at random, there is only a
probability of $1/4$ that the test lies. Of course, we want a smaller
probability, so we repeat the test many times. By using $\lambda$
iterations for different random bases, we bring the error probability
down to $1/4^\lambda$. Observe that this decreases exponentially in
$\lambda$! To give some concrete numbers, for $\lambda=1024$, we have
that $$\frac{1}{4^\lambda}\approx 3.094346\cdot 10^{-67},$$ and this
looks like a failure probability that we can live with.

Asymptotically, we have an algorithm that runs $\lambda$ iterations, and
each of these is computing $O(\lambda)$ exponentiations, so in total we
have a running cost of $O(\lambda^2)$ modular exponentiations.

This test is known as the *Miller--Rabin primality test*, and it is in
essence what is used in practice to check for primality, due to its high
efficiency and almost-perfect reliability.

If you are still concerned about the error probability, there are some
alternatives that *never* produce erroneous results, while still running
in polynomial time. However, they are much slower than the Miller--Rabin
test, which is why they are not often used in practice.[^43]

# Polynomial interpolation

We start from a very well-known idea: two different points in a plane
determine a line. Or equivalently: given two different points in a
plane, there is exactly one line that contains both.

More formally, let $(x_0,y_0)$ and $(x_1,y_1)$ be points in
$\mathbb{R}^2$, such that $x_0\neq x_1$.[^44] Then there is a unique
polynomial $p(X)$ of degree at most $1$ such that
$$p(x_0)=y_0,\qquad \text{and} \qquad p(x_1)=y_1.$$ Although can find
this polynomial by finding the slope of the line, we will show another
approach that yields the same result, that we will later generalize to a
higher number of points. Consider the following simpler problem.

::: {.problem}
**Problem 7**. *Given $x_0,x_1\in\mathbb{R}$, find a polynomial
$\ell_0(X)$ of degree at most $1$ such that
$$\ell_0(x_0)=1,\qquad\text{ and }\ell_0(x_1)=0.$$*
:::

The simplest idea to achieve the second condition is to make $x_1$ a
root of the polynomial. Note that, since the degree must be at most $1$,
the polynomial will have at most one root, making $x_1$ the only root.
Let us then consider the polynomial $$\hat\ell_0(X)=X-x_1.$$ Clearly
this polynomial satisfies the second condition, but it fails at the
first, since $$\hat\ell_0(x_1)=x_0-x_1,$$ which might be different from
$1$. Thus, to ensure the second condition, we rescale the polynomial by
dividing by the value at $x_0$: $$\ell_0(X)=\frac{X-x_1}{x_0-x_1}.$$
This new polynomial is indeed a solution for the problem. Similarly, we
can find another polynomial $\ell_1(X)$ that is $0$ at $x_0$ and $1$ at
$x_1$. Finally, the solution to our original problem will be
$$p(X)=y_0\ell_0(X)+y_1\ell_1(X).$$

::: {.exercise}
*Check that this polynomial has degree at most $1$,
that $p(x_0)=y_0$ and $p(x_1)=y_1$.*
:::

It is easy to check that this polynomial verifies the conditions above.
We say that this line is an *interpolation* of the points. This idea can
be generalized to polynomial of any degree. That is, there is a unique
parabola (polynomial of degree $2$) that contains three given points in
the plane. There is a degree-$3$ polynomial that contains four given
points, and so on. Moreover, this also works over a finite field
$\mathbb{F}_p$ for any prime $p$, when the operations are considered
modulo $p$ and division is inversion modulo $p$.

::: {.proposition}
*Let $n\in\mathbb{N}$, and let $p\in\mathbb{Z}$ be a
prime number. Consider $n+1$ pairs $(x_i,y_i)$, for $i=0,\dots,n$, of
points in $\mathbb{F}_p$, such that the $x_i$ are pairwise different.
Then, there is a unique polynomial $p(X)$ of degree at most $n$ such
that $$p(x_i)=y_i,\qquad\text{ for all }i=0,\dots,n.$$ The polynomial
$p(X)$ is called the* Lagrange interpolation polynomial *of degree $n$
relative to the points $(x_i,y_i)$ for* $i=0,\dots,n$.
:::

The strategy to build this polynomial explicitly is to generalize what
we have done before for two points. That is, we first want to solve the
following problem.

::: {.problem}
**Problem 8**. *Given $x_0,\dots,x_n\in\mathbb{F}_p$, find polynomials
$\ell_i(X)$, for $i=0,\dots,n$, of degree at most $n$ such that
$$\ell_i(x_i)=1,\qquad\text{ and }\ell_i(x_j)=0, \qquad \text{ for all }j\neq i.$$*
:::

That is, we want polynomials such that each of them is $1$ at at one of
the points and $0$ at the rest of the points. For the polynomial
$\ell_i(X)$, we ensure that all $x_j\neq x_i$ are roots of the
polynomial. We consider: $$\hat\ell_i(X)=\prod_{j\neq i}(X-x_j).$$ This
polynomial has degree $n$, since it has $n$ factors, each of degree $1$.
Moreover, it clearly satisfies that $\hat\ell_i(x_j)=0$ if $j\neq i$.
However, we still run into the problem that it produces the wrong value
at $x_i$. Indeed, $$\hat\ell_i(x_i)=\prod_{j\neq i}(x_i-x_j).$$ We solve
the issue by dividing the polynomial by this value, and thus we consider
the polynomial
$$\ell_i(X)=\frac{\prod_{j\neq i}(X-x_j)}{\prod_{j\neq i}(x_i-x_j)}=\prod_{j\neq i}\frac{(X-x_j)}{x_i-x_j}.$$
The polynomials $\ell_i(X)$ are known as the *Lagrange basis
polynomials*, as we can now use them as a basis for building any
interpolation polynomial relative to the points $x_0,\dots,x_n$. Indeed,
we consider the linear combination $$p(X)=\sum_{i=0}^n y_i\ell_i(X).$$
It is easy to check that this polynomial satisfies that $$p(x_i)=y_i$$
for all $i=0,\dots,n$. Moreover, since the degree of the $\ell_i(X)$ is
$n$, and addition of the polynomials or multiplication by constant do
not increase the degree (although they might reduce it), the polynomial
$p(X)$ will have degree at most $m$. Therefore, this is the Lagrange
interpolation polynomial that we were looking for.

::: {.exercise}
*Compute the Lagrange interpolation polynomial
corresponding to the points $(0,1),(1,-1),(2,2),(3,8)$.*
:::

# Refreshers {#refreshers}

## Set notation

A *set* is a well-defined collection of objects. Such objects are said
to be *elements* of the set, or that they *belong* to the set. For
example, the set of the vowels is
$$V=\{\text{a},\text{e},\text{i},\text{o},\text{u}\}.$$ In the above
line we are giving a name to the set, $V$, and we are specifying the
list of its elements: a, e, i, o and u. When describing a set
explicitly, we write the list of its elements between braces
$\{\hspace{0.1cm} \}$.

Two sets are equal if they have exactly the same elements. An element
cannot belong 'twice' to a set. Therefore, we can say that
$$V=\{\text{a},\text{e},\text{i},\text{o},\text{u}\}=\{\text{u},\text{o},\text{i},\text{e},\text{a}\}=\{\text{a},\text{a},\text{e},\text{e},\text{e},\text{i},\text{o},\text{u}\}.$$
The symbol $\in$ indicates membership of an element in a set. For
example, we can write a $\in V$, because a belongs to the set $V$. On
the other hand, we have that b $\not\in V$, since b is not any of the
elements of $V$.

There are some number sets that show up very frequently, so we give them
special names.

-   $\mathbb{N}$: set of natural numbers.

-   $\mathbb{Z}$: set of integer numbers.

-   $\mathbb{Q}$: set of rational numbers.

-   $\mathbb{R}$: set of real numbers.

-   $\mathbb{C}$: set of complex numbers.

Observe that, unlike in the prior examples, all these sets are
*infinite*, that is, they have an infinite number of elements. Obviously
we cannot describe an infinite set explicitly, as we have done with the
set of vowels. What we can do is refer to other sets that we have
already defined and define restrictions on them. For example, we can
define the set of even natural numbers as the set of natural numbers
that are multiples of $2$. Formally, we write this set as
$$\{n\in \mathbb{N}\mid  n=2k\text{ for some }k\in\mathbb{N}\}.$$ Here
we have introduced some new notation. Instead of explicitly enumerating
all the elements of a set, we give some conditions. We read the
description above as 'the set of elements of the form indicated on the
left of the vertical line, such that they verify the condition on the
right'. In this case, the set of natural numbers such that they are of
the form $2k$ for some natural value of $k$. Similarly, we can define
the set of all real numbers greater than $5$ in the following way:
$$\{x\in \mathbb{R}\mid  x>5\}.$$

We denote the size (number of elements) of a set $S$ by $\#S$.

We can produce new sets by considering the *product* of known sets:
given two sets $S,T$, we define the set $S\times T$ as the set whose
elements are the pairs $(s,t)$, where $s\in S$ and $t\in T$. For
example,
$$\{0,1\}\times\{0,1,2\}=\{(0,0),(0,1),(0,2),(1,0),(1,1),(1,2)\}.$$
Observe that $\#(X\times Y)=\#X\cdot\#Y$. It is easy to generalize this
definition to products of three or more sets. In particular, given a set
$S$, we define
$$S^n=\underbrace{S\times S\times \dots \times S}_{n\text{ times}}$$ In
this course, we will often use the set $\{0,1\}$ of possible bits, the
set $\{0,1\}^\ell$ of possible bitstrings of length $\ell$, and the set
$\{0,1\}^*$ of bitstrings of any length. Observe that
$$\#\{0,1\}=2,\qquad\#\{0,1\}^\ell=2^\ell,\qquad \#\{0,1\}^*=\infty.$$

::: {.exercise}
*Write these sets using implicit notation:*

-   *The set of complex numbers with real part equal to $1$.*

-   *The set of pairs, where the first component of the pair is a
    rational number and the second component is an odd natural number.*

-   *The set of bitstrings of length $10$ with exactly $5$ zeros.*
:::

## Probability theory

We will deal with probability distributions over finite sets. In
particular, recall that the *uniform distribution* over a set $S$ is the
probability distribution that assigns the same probability $1/\#S$ to
each element of $S$. We denote sampling an element $x$ from the uniform
distribution over $S$ by $$x\gets S.$$ For example, the notation
$$\mathbf b\gets \{0,1\}^{128}$$ means that $\mathbf b$ is a uniformly
random bitstring of length $128$.

Given an event $A$, we denote the *probability of $A$* by $\Pr[A]$.
Given two events $A,B$, we denote the *probability of $A$ conditioned on
$B$* by $\Pr[A|B]$, and if $\Pr[B]\neq0$ we have that
$$\Pr[A|B]=\frac{\Pr[A\cap B]}{\Pr[B]},$$ where $\Pr[A\cap B]$ means the
probability of both $A$ and $B$ happening. Recall that, if $A$ and $B$
are independent events, then $$\Pr[A\cap B]=\Pr[A]\cdot\Pr[B].$$

::: {.exercise}
*Compute the probability of a random bitstring of
length $4$ being the string $1110$.*
:::

## Asymptotic notation

Asymptotic notation allows us to easily express the *asymptotic
behaviour* of functions, that is, how the function changes for
arbitrarily large inputs, with respect to some other function. For
example, take the functions defined by $$f(x)=x,\qquad\qquad g(x)=x^2.$$
Both tend to infinity as $x$ tends to infinity, but the second does it
"faster". More precisely, let $$f,g:\mathbb{N}\rightarrow\mathbb{N}$$ be
two functions. We write $$f(x)=O(g(x))$$ when there is some
$N,M\in\mathbb{N}$ such that, for all $x>N$, we have
$$f(x)\leq M\cdot g(x).$$ We read this as "$f$ is big-O of $g$". Then,
back to the initial example, we can say write $$x=O(x^2).$$ Note that,
because the big-O notation "absorbs" constants into $M$, we can write
$$2x=O(x),$$ even though $2x\geq x$ for $x\in\mathbb{N}$.

Big-O notation is useful for representing bounds on the growth speed of
a function. By saying, for example, that a function $f$ satisfies
$$f(x)=O(x^3),$$ we are saying that, at worst, the function $f$ grows as
fast as a cubic polynomial. Therefore, in particular, it will not grow
as fast as a polynomial of degree $4$, or an exponential function like
$2^x$.

We recall that logarithmic functions grow slower than polynomials of any
degree, and polynomials of any degree grow slower than exponential
functions. Given two polynomials of different degrees, the one with the
higher degree grows faster.

::: {.exercise}
*Decide whether each of these statements is true or
false.*

-   *$10^{10}x^3=O(x^4)$.*

-   *$10^x=O(x^4)$.*

-   *$\log(x)=O(x\log x)$.*

-   *$4^x=O(2^x)$.*
:::

## Polynomial division {#polydiv}

Consider two polynomials $$\begin{split}
    & A(X)=a_kX^k+a_{k-1}X^{k-1}+\dots+a_1X+a_0, \\
    & B(X)=b_{\ell}X^{\ell}+b_{\ell-1}X^{\ell-1}+\dots+b_1X+b_0, \\
\end{split}$$ where $k\geq\ell$. Then, we have the following result.

```{proposition, label=polydiv}
*Let $A(X)$ and $B(X)$ be the polynomials defined
above. Then, there exist polynomials $Q(X)$ and $R(X)$ such that
$$A(X)=B(X)Q(X)+R(X),$$ where the degree of $R$ is smaller than $\ell$.
In this case, $Q$ is called the* quotient *and $R$ is called the* remainder *of the division.*
```

The algorithm is very similar to the algorithm of integer division. The
idea is to try to find factors such that, when multiplied by $B(X)$,
allow us to remove the highest-degree term left in $A(X)$, and use these
terms to build $Q(X)$. Although very simple, the idea is cumbersome to
explain for general polynomials, so we illustrate it with an example.
Let $$A(X)=3X^5+X^4+2X^2+7,\qquad B(X)=X^3+X^2+X.$$ We arrange them in
the usual diagram: $$\begin{array}{rrrrrrl}
    3X^5 & +x^4 & & +2x^2 & & +7 \qquad &  |\quad X^3+X^2+X\\
     &&&&&& \overline{\phantom{|\quad  X^3+X^2+X}} \\
\end{array}$$ We want to remove $3X^5$, so we try to find a factor
$f(X)$ such that $f(X)X^3=3X^5$. Thus, we choose $f(X)=3X^2$, multiply
it by $B(X)$, and subtract the result from $A(X)$, obtaining a
lower-degree polynomial: $$\begin{array}{rrrrrrl}
    3X^5 & +X^4 & & +2X^2 & & +7 \qquad &  |\quad X^3+X^2+X\\
     &&&&&& \overline{\phantom{|\quad  X^3+X^2+X}} \\
      & -2X^4 & -3X^3 & +2X^2 &  &+ 7 \qquad & 3X^2\\
\end{array}$$ We now look at the new polynomial, $-2X^4-3X^3+2X^2+7$,
and again try to remove the highest-degree monomial, which we achieve by
multiplying $X^3$ by $-2X$. $$\begin{array}{rrrrrrl}
    3X^5 & +X^4 & & +2X^2 & & +7 \qquad &  |\quad X^3+X^2+X\\
     &&&&&& \overline{\phantom{|\quad  X^3+X^2+X}} \\
      & -2X^4 & -3X^3 & +2X^2 &  &+ 7 \qquad & 3X^2-2X\\
      &&&&&& {\phantom{|\quad  X^3+X^2+X}} \\
     & & -X^3 + 4X^2 &&&+ 7 \qquad & \\
\end{array}$$ Again, we want to remove the term $-X^3$, so we multiply
$B(X)$ by $-1$: $$\begin{array}{rrrrrrl}
    3X^5 & +X^4 & & +2X^2 & & +7 \qquad &  |\quad X^3+X^2+X\\
     &&&&&& \overline{\phantom{|\quad  X^3+X^2+X}} \\
      & -2X^4 & -3X^3 & +2X^2 &  &+ 7 \qquad & 3X^2-2X-1\\
      &&&&&& {\phantom{|\quad  X^3+X^2+X}} \\
     & & -X^3 + 4X^2 &&&+ 7 \qquad & \\
     &&&&&& {\phantom{|\quad  X^3+X^2+X}} \\
     & & & 5X^2 & +X & +7 \qquad & \\
\end{array}$$ Since the degree of the current dividend is smaller than
the degree of the divisor, we are done. We conclude that the quotient of
the division of $A(X)$ by $B(X)$ is $$3X^2-2X-1,$$ and the remainder is
$$5X^2+X+7.$$ Note that the above example works in $\mathbb{Q}$, but in
general operations on the coefficients must take into account which
field we are in. For example, if our coefficients are in $\mathbb{F}_5$,
we cannot "divide" by $3$, but we can find the inverse of $3$ modulo
$5$, and we must ensure that every coefficient is reduced modulo $5$.

[^42]: In an abstract algebra context, the definition of a ring is more
    general, and what we are defining is known as a *commutative ring
    with unity*. Nevertheless, we stick to this definition for
    simplicity, since it is the only type of ring relevant to us.

[^43]: Agrawal, M., Kayal, N., & Saxena, N. (2004). PRIMES is in P.
    *Annals of mathematics*, 781-793.

[^44]: We require the $x$-coordinates to be different because we want to
    formulate this in terms of functions, and a function cannot have two
    outputs for the same input.
